{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ANN implementation using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Forward and Backward Propagation in a 1-Layer Neural Network:</b>\n",
    "    Let's implement a basic example with 1 input and 1 output neuron. This will help us understand both forward and backward propagation intuitively.\n",
    "\n",
    "* <u><b>Forward Propagation:</b></u>\n",
    "\n",
    "    In forward propagation, you pass the input through the neural network layers to get the output.\n",
    "\n",
    "* <u><b>Backward Propagation:</b></u>\n",
    "\n",
    "    In backward propagation, you calculate the gradients and update the weights using the chain rule of calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Input and output\n",
    "X = np.array([[0], [1]])  # Input: 0 and 1\n",
    "y = np.array([[0], [1]])  # Expected output: same values for simplicity\n",
    "\n",
    "# Initialize random weights\n",
    "np.random.seed(1)\n",
    "w = np.random.randn(1, 1)  # Single weight\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Training loop (forward and backward propagation)\n",
    "for epoch in range(10000):  # Train for 10,000 iterations\n",
    "    # Forward propagation\n",
    "    z = np.dot(X, w)  # Weighted sum\n",
    "    output = sigmoid(z)  # Activation function\n",
    "    \n",
    "    # Compute error (loss)\n",
    "    error = y - output\n",
    "    \n",
    "    # Backward propagation (gradient descent)\n",
    "    output_derivative = sigmoid_derivative(output)\n",
    "    d_w = np.dot(X.T, error * output_derivative)  # Gradient of the weight\n",
    "    \n",
    "    # Update weights\n",
    "    w += lr * d_w\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Error: {np.mean(np.abs(error))}\")\n",
    "\n",
    "# Final output after training\n",
    "print(\"Final weights:\", w)\n",
    "print(\"Predicted output after training:\", sigmoid(np.dot(X, w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Explanation:</b>\n",
    "* <u><b>Forward Propagation:</b></u>\n",
    "\n",
    "    The input ùëã is multiplied by the weight ùë§, and the result is passed through the sigmoid activation function to produce the output.\n",
    "\n",
    "* <u><b>Backward Propagation:</b></u>\n",
    "\n",
    "    The error is calculated by subtracting the output from the expected value ùë¶.\n",
    "    \n",
    "    The gradient of the error with respect to the weights is computed, which tells how much the weights should be adjusted.\n",
    "    \n",
    "    The weights are updated using gradient descent: ùë§ = ùë§ + learning¬†rate √ó gradient.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
