{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "/home/vpsr/Desktop/python/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': ''}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpsr/Desktop/python/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"? Machine learning is a type of artificial intelligence (AI) that allows computer systems to automatically learn and improve from experience without being explicitly programmed. It involves the use of algorithms and statistical models to analyze data, learn from it, and make predictions or decisions based on that learning. Machine learning is used in a wide range of applications, including image and speech recognition, natural language processing, and recommendation systems. It is a rapidly growing field that is expected to have a significant impact on many industries in the coming years.\\n\\nWhat are some examples of machine learning applications? Some examples of machine learning applications include:\\n\\n* Image recognition: Machine learning algorithms can be trained to recognize images and classify them based on their content. For example, a machine learning model could be trained to identify a cat in a photograph.\\n* Speech recognition: Machine learning algorithms can be used to transcribe spoken words into text. For example, a voice assistant like Siri or Alexa uses machine learning to understand and respond to spoken commands.\\n* Natural language processing: Machine learning algorithms can be used to understand and generate human language. For example, a chatbot could use machine learning to respond to customer inquiries in a natural, conversational way.\\n* Recommendation systems: Machine learning algorithms can be used to recommend products or content to users based on their preferences and past behavior. For example, a music streaming service like Spotify could use machine learning to recommend songs to a user based on their listening history.\\n\\nWhat are some challenges with machine learning? There are several challenges associated with machine learning, including:\\n\\n* Data quality: Machine learning models are only as good as the data they are trained on. If the data is of poor quality or biased, the model's predictions may be inaccurate.\\n* Lack of transparency: Machine learning models can be complex and difficult to understand, making it difficult to determine why they are making certain predictions. This lack of transparency can be a problem in applications where it is important to be able to explain the reasoning behind a decision.\\n* Bias: Machine learning models can be biased if they are trained on data that is not representative of the population they are intended to serve. For example, a machine learning model trained on data from a predominantly white population may not perform as well on data from a predominantly black population.\\n* Privacy concerns: Machine learning models often require large amounts of data to train effectively, which can raise privacy concerns if\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpsr/Desktop/python/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' and how can it be used for content creation? Generative AI refers to a type of artificial intelligence that can create new content, such as text, images, music, and even videos, by learning patterns and structures from existing data. It\\'s called \"generative\" because it generates new content, rather than just processing or analyzing existing content.\\n\\nFor content creation, generative AI can be used in a variety of ways. For example, it can be used to write articles, create blog posts, or generate social media content. It can also be used to generate creative writing, such as poetry or short stories. In the realm of visual content, generative AI can create images, illustrations, or even 3D models. For music, it can compose new melodies, chords, or even entire songs.\\n\\nGenerative AI works by learning patterns from large amounts of data. This data can be anything from books, articles, and social media posts for text generation, to images, videos, and sound files for visual and auditory content. The AI then uses these learned patterns to create new content that is similar in style and structure to the original data.\\n\\nWhile generative AI has the potential to revolutionize content creation, it also raises some ethical and quality concerns. For example, it\\'s important to ensure that the AI is not plagiarizing existing content, and that it\\'s producing high-quality, engaging, and original content. Additionally, there are concerns about the potential misuse of AI for creating misleading or harmful content.\\n\\nIn conclusion, generative AI is a powerful tool for content creation, with the potential to automate and streamline many content creation tasks. However, it\\'s important to use it responsibly and ethically, and to ensure that it\\'s producing high-quality, original content.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18077/3735178259.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
      "/home/vpsr/Desktop/python/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nIndia won the 2011 Cricket World Cup, defeating Sri Lanka by six wickets in the final, played at Wankhede Stadium in Mumbai, India on 2 April 2011. This was India's second Cricket World Cup title and first since the 1983 Cricket World Cup.\\n\\nWho won the World Cup 2007?\\n\\nThe 2007 Cricket World Cup was the ninth Cricket World Cup tournament, organised by the International Cricket Council (ICC). It was held in the West Indies between 13 March and 28 April 2007. Australia won the tournament, defeating Sri Lanka by 53 runs in the final at the Kensington Oval in Bridgetown, Barbados.\\n\\nWho won the World Cup 2015?\\n\\nThe 2015 Cricket World Cup was the eleventh Cricket World Cup tournament, organised by the International Cricket Council (ICC). The tournament was held in Australia and New Zealand between 14 February and 29 March 2015. Australia won the tournament, defeating New Zealand by seven wickets in the final at the Melbourne Cricket Ground.\\n\\nWho won the World Cup 2019?\\n\\nThe 2019 Cricket World Cup was the twelfth Cricket World Cup tournament, organised by the International Cricket Council (ICC). The tournament was held in England and Wales between 30 May and 14 July 2019. England won the tournament, defeating New Zealand by a single boundary in the final at Lord's.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18077/2593296450.py:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceBgeEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.028416551649570465,\n",
       " 0.012183290906250477,\n",
       " 0.027443939819931984,\n",
       " -0.054828692227602005,\n",
       " 0.02423887699842453,\n",
       " 0.0007662862190045416,\n",
       " 0.06783366203308105,\n",
       " 0.016348298639059067,\n",
       " -0.01895073801279068,\n",
       " 0.012542907148599625,\n",
       " 0.021565014496445656,\n",
       " -0.08793039619922638,\n",
       " 0.0006460539880208671,\n",
       " 0.033270783722400665,\n",
       " 0.0054637533612549305,\n",
       " -0.06037640944123268,\n",
       " 0.05042264237999916,\n",
       " 0.004434807226061821,\n",
       " 0.0009598658652976155,\n",
       " 0.0017405833350494504,\n",
       " 0.0032988202292472124,\n",
       " 0.03167250379920006,\n",
       " -0.048807479441165924,\n",
       " -0.04481915012001991,\n",
       " 0.07132111489772797,\n",
       " -0.00751084927469492,\n",
       " -0.0011259291786700487,\n",
       " -0.01580117829144001,\n",
       " -0.029402390122413635,\n",
       " -0.17224565148353577,\n",
       " -0.03189520537853241,\n",
       " -0.0016291735228151083,\n",
       " 0.018104981631040573,\n",
       " 0.015315393917262554,\n",
       " -0.020729562267661095,\n",
       " -0.008872998878359795,\n",
       " -0.001282262266613543,\n",
       " 0.027276858687400818,\n",
       " -0.010114247910678387,\n",
       " 0.012621620669960976,\n",
       " -0.007077870890498161,\n",
       " -0.016693195328116417,\n",
       " 0.04085586220026016,\n",
       " 0.0239383727312088,\n",
       " -0.02008151076734066,\n",
       " 0.02868114411830902,\n",
       " -0.019400758668780327,\n",
       " -0.014618179760873318,\n",
       " 0.017379634082317352,\n",
       " 0.0041640824638307095,\n",
       " 0.06415649503469467,\n",
       " 0.047683048993349075,\n",
       " 0.0018365011783316731,\n",
       " -8.071921183727682e-05,\n",
       " 0.016596777364611626,\n",
       " 0.011124166660010815,\n",
       " 0.0696943923830986,\n",
       " 0.05182050168514252,\n",
       " 0.05568530037999153,\n",
       " 0.05551542714238167,\n",
       " 0.0005039448151364923,\n",
       " 0.041870586574077606,\n",
       " -0.15344084799289703,\n",
       " 0.05180780589580536,\n",
       " 0.006689810194075108,\n",
       " -0.0316707082092762,\n",
       " -0.009105006232857704,\n",
       " -0.051604703068733215,\n",
       " 0.042508576065301895,\n",
       " 0.028200028464198112,\n",
       " -0.010748126544058323,\n",
       " 0.02240578643977642,\n",
       " 0.044395510107278824,\n",
       " 0.004115525167435408,\n",
       " 0.018998468294739723,\n",
       " -0.004357169847935438,\n",
       " 0.04762760177254677,\n",
       " 0.01182462926954031,\n",
       " 0.008164589293301105,\n",
       " 0.008177279494702816,\n",
       " -0.009698745794594288,\n",
       " -0.014260295778512955,\n",
       " 0.011409704573452473,\n",
       " -0.07362113147974014,\n",
       " -0.054395224899053574,\n",
       " -0.05703964829444885,\n",
       " -0.003608556929975748,\n",
       " 0.002666121581569314,\n",
       " 0.02378247305750847,\n",
       " 0.015376229770481586,\n",
       " -0.0702037364244461,\n",
       " -0.031300388276576996,\n",
       " -0.0031142826192080975,\n",
       " -0.015812167897820473,\n",
       " -0.03791399300098419,\n",
       " -0.02592191845178604,\n",
       " 0.018168412148952484,\n",
       " -0.038824599236249924,\n",
       " -0.05674506723880768,\n",
       " 0.5792059898376465,\n",
       " -0.052788328379392624,\n",
       " 0.02071639709174633,\n",
       " 0.06794389337301254,\n",
       " -0.045416541397571564,\n",
       " 0.011642451398074627,\n",
       " -0.021571749821305275,\n",
       " 0.020341720432043076,\n",
       " -0.027448948472738266,\n",
       " -0.04558897390961647,\n",
       " -0.029443571344017982,\n",
       " -0.02366252988576889,\n",
       " -0.03431526571512222,\n",
       " 0.0019388716900721192,\n",
       " -0.07095137983560562,\n",
       " 0.034556321799755096,\n",
       " -0.03055894374847412,\n",
       " 0.03907860070466995,\n",
       " -0.029707323759794235,\n",
       " -0.0008282953640446067,\n",
       " -0.012159359641373158,\n",
       " -0.01827280782163143,\n",
       " 0.02548649162054062,\n",
       " -0.004461662378162146,\n",
       " 0.01633528061211109,\n",
       " 0.019126519560813904,\n",
       " -0.054832085967063904,\n",
       " 0.0276359710842371,\n",
       " -0.004757678601890802,\n",
       " 0.059001706540584564,\n",
       " -0.001694467500783503,\n",
       " 0.008015024475753307,\n",
       " -0.03772684186697006,\n",
       " -0.09893041849136353,\n",
       " -0.022574396803975105,\n",
       " -0.03760464861989021,\n",
       " -0.0021698649507015944,\n",
       " 0.003244602121412754,\n",
       " -0.019202543422579765,\n",
       " -0.008631229400634766,\n",
       " -0.048023074865341187,\n",
       " 0.008696705102920532,\n",
       " -0.0951610878109932,\n",
       " -0.03496047481894493,\n",
       " -0.04360802471637726,\n",
       " -0.00034401085576973855,\n",
       " -0.010173649527132511,\n",
       " -0.03099953383207321,\n",
       " 0.024309692904353142,\n",
       " -0.02040203846991062,\n",
       " 0.03113941103219986,\n",
       " 0.0008811188745312393,\n",
       " 0.013916457071900368,\n",
       " -0.031196242198348045,\n",
       " -0.03715403750538826,\n",
       " 0.00402966421097517,\n",
       " 0.014799799770116806,\n",
       " 0.04318894073367119,\n",
       " 0.03875482827425003,\n",
       " 0.013851992785930634,\n",
       " 0.019797878339886665,\n",
       " 0.010267076082527637,\n",
       " -0.00543410936370492,\n",
       " -0.014299212023615837,\n",
       " 0.027637837454676628,\n",
       " 0.009802635759115219,\n",
       " -0.1355028599500656,\n",
       " -0.017139775678515434,\n",
       " 0.017617065459489822,\n",
       " 0.023132218047976494,\n",
       " 0.0017590150237083435,\n",
       " 0.030889401212334633,\n",
       " 0.03991870582103729,\n",
       " -0.013684182427823544,\n",
       " 0.024816546589136124,\n",
       " 0.05405019596219063,\n",
       " 0.017761176452040672,\n",
       " -0.018475057557225227,\n",
       " 0.02595536969602108,\n",
       " -0.006377536803483963,\n",
       " -0.01658732257783413,\n",
       " 0.03784802183508873,\n",
       " -0.027290061116218567,\n",
       " -0.05284581333398819,\n",
       " -0.03803319111466408,\n",
       " 0.051911093294620514,\n",
       " -0.00755709083750844,\n",
       " -0.03180531784892082,\n",
       " 0.013284181244671345,\n",
       " -0.02772372215986252,\n",
       " 0.056306563317775726,\n",
       " 0.0030418727546930313,\n",
       " 0.05332484468817711,\n",
       " -0.05791125446557999,\n",
       " -0.01132582500576973,\n",
       " -0.031172025948762894,\n",
       " 0.025608690455555916,\n",
       " 0.03389059379696846,\n",
       " -0.0010284638265147805,\n",
       " 0.015864891931414604,\n",
       " 0.01059520710259676,\n",
       " -0.027037784457206726,\n",
       " -0.0009308481821790338,\n",
       " -0.048152245581150055,\n",
       " 0.02817925624549389,\n",
       " 0.010320611298084259,\n",
       " 0.06662959605455399,\n",
       " -0.016558179631829262,\n",
       " -0.0044313836842775345,\n",
       " 0.03823427855968475,\n",
       " -0.023408185690641403,\n",
       " -0.03558176010847092,\n",
       " -0.05829068273305893,\n",
       " -0.011181475594639778,\n",
       " -0.017684556543827057,\n",
       " -0.016141317784786224,\n",
       " -0.0342453233897686,\n",
       " -0.025139523670077324,\n",
       " 0.039396680891513824,\n",
       " -0.023658234626054764,\n",
       " -0.0077250259928405285,\n",
       " -0.005098923575133085,\n",
       " -0.03523436188697815,\n",
       " -0.014076855033636093,\n",
       " -0.223260298371315,\n",
       " -0.03147134929895401,\n",
       " -0.0012905760668218136,\n",
       " -0.0017200281145051122,\n",
       " -0.007846028544008732,\n",
       " -0.058023229241371155,\n",
       " 0.04617457836866379,\n",
       " 0.024552633985877037,\n",
       " 0.07320840656757355,\n",
       " 0.01726832054555416,\n",
       " 0.04761206731200218,\n",
       " 0.013473288156092167,\n",
       " -0.00551604712381959,\n",
       " -0.01435783226042986,\n",
       " -0.009674306027591228,\n",
       " 0.04878249019384384,\n",
       " 0.03053811378777027,\n",
       " -0.02499394677579403,\n",
       " 0.021486220881342888,\n",
       " 0.017639808356761932,\n",
       " 0.05313890427350998,\n",
       " 0.013485017232596874,\n",
       " -0.02322598733007908,\n",
       " -0.0214039646089077,\n",
       " 0.02607535943388939,\n",
       " 0.0020291768014431,\n",
       " 0.12753744423389435,\n",
       " 0.08316836506128311,\n",
       " 0.04408949986100197,\n",
       " -0.02670356072485447,\n",
       " 0.005522006191313267,\n",
       " -0.009294882416725159,\n",
       " 0.02007431723177433,\n",
       " -0.09684177488088608,\n",
       " -0.02470390312373638,\n",
       " 0.02508697845041752,\n",
       " 0.002088637789711356,\n",
       " -0.04489404335618019,\n",
       " -0.07861137390136719,\n",
       " -0.004376350436359644,\n",
       " -0.06590452045202255,\n",
       " 0.014689420349895954,\n",
       " -0.05764184892177582,\n",
       " -0.07152026891708374,\n",
       " -0.06232648715376854,\n",
       " 0.003431654302403331,\n",
       " -0.04606551304459572,\n",
       " 0.04530090093612671,\n",
       " -0.02676226757466793,\n",
       " 0.03401090204715729,\n",
       " 0.04547379910945892,\n",
       " -0.028179261833429337,\n",
       " 0.005011794622987509,\n",
       " 0.009630824439227581,\n",
       " -0.03030560351908207,\n",
       " -0.0361248143017292,\n",
       " -0.013626988045871258,\n",
       " -0.032653722912073135,\n",
       " -0.04467754065990448,\n",
       " 0.010642195120453835,\n",
       " -0.02748635970056057,\n",
       " -0.024565119296312332,\n",
       " -0.02474776655435562,\n",
       " 0.05361955612897873,\n",
       " 0.020789938047528267,\n",
       " 0.01946849375963211,\n",
       " 0.05324118584394455,\n",
       " -0.014002451673150063,\n",
       " 0.021243266761302948,\n",
       " -0.04957321286201477,\n",
       " -0.0085225785151124,\n",
       " 0.007852859795093536,\n",
       " -0.05719393864274025,\n",
       " -0.027550652623176575,\n",
       " 0.005300882738083601,\n",
       " 0.04007291421294212,\n",
       " 0.019597919657826424,\n",
       " -0.04519733414053917,\n",
       " 0.032435812056064606,\n",
       " -0.012342444621026516,\n",
       " 0.034314434975385666,\n",
       " 0.021102122962474823,\n",
       " 0.039846502244472504,\n",
       " 0.03166380152106285,\n",
       " -0.033590223640203476,\n",
       " 0.03164786845445633,\n",
       " -0.0033045089803636074,\n",
       " 0.004641844425350428,\n",
       " 0.037589360028505325,\n",
       " -0.05924459919333458,\n",
       " 0.0070283422246575356,\n",
       " 0.0038086853455752134,\n",
       " -0.025788893923163414,\n",
       " -0.021203359588980675,\n",
       " 0.022691216319799423,\n",
       " -0.02177296206355095,\n",
       " -0.27963775396347046,\n",
       " 0.007267413195222616,\n",
       " 0.021072009578347206,\n",
       " 0.04519743472337723,\n",
       " -0.020534468814730644,\n",
       " 0.02431371435523033,\n",
       " -0.0006136976880952716,\n",
       " -0.011857017874717712,\n",
       " -0.03296778351068497,\n",
       " 0.03584315627813339,\n",
       " 0.0312817320227623,\n",
       " 0.06373955309391022,\n",
       " 0.046547841280698776,\n",
       " -0.014470524154603481,\n",
       " 0.01586972549557686,\n",
       " 0.03397125378251076,\n",
       " 0.018059581518173218,\n",
       " 0.002298751613125205,\n",
       " 0.016549862921237946,\n",
       " -0.021714895963668823,\n",
       " -0.03485998883843422,\n",
       " -0.0008649306837469339,\n",
       " 0.15126043558120728,\n",
       " -0.02453676424920559,\n",
       " 0.03067120909690857,\n",
       " -0.007318185642361641,\n",
       " -0.006135446019470692,\n",
       " 0.06415148079395294,\n",
       " 0.0160214900970459,\n",
       " -0.03636440634727478,\n",
       " 0.019898606464266777,\n",
       " -0.021172329783439636,\n",
       " 0.048294153064489365,\n",
       " -0.044780999422073364,\n",
       " 0.0476338192820549,\n",
       " 0.0007749302894808352,\n",
       " -0.005927938502281904,\n",
       " 0.06154269725084305,\n",
       " 0.023968406021595,\n",
       " 0.013305027969181538,\n",
       " 0.02268449403345585,\n",
       " 0.014538086019456387,\n",
       " -0.052159059792757034,\n",
       " -0.03274965286254883,\n",
       " 0.08583345264196396,\n",
       " -0.003724820679053664,\n",
       " 0.0013494276208803058,\n",
       " 0.040919892489910126,\n",
       " 0.011659654788672924,\n",
       " 0.05843619629740715,\n",
       " -0.022286195307970047,\n",
       " -0.011520685628056526,\n",
       " 0.004705725237727165,\n",
       " 0.0471826009452343,\n",
       " -0.0019179026130586863,\n",
       " 0.033009372651576996,\n",
       " -0.035050563514232635,\n",
       " -0.020736578851938248,\n",
       " -0.009222174994647503,\n",
       " 0.014618266373872757,\n",
       " 0.006456067319959402,\n",
       " 0.0010978507343679667,\n",
       " 0.010224021971225739,\n",
       " 0.0853722095489502,\n",
       " 0.03883953019976616]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
